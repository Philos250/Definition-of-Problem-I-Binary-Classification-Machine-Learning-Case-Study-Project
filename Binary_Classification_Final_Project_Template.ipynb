{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de projet de modélisation prédictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'apprentissage automatique appliqué est une compétence empirique. Vous ne pouvez pas vous améliorer en lisant des livres et des articles. Vous devez pratiquer. Dans ce document, vous découvrirez le modèle de projet d'apprentissage automatique simple en six étapes que vous pouvez utiliser pour démarrer rapidement votre projet en Python. En suivant ces étapes, vous saurez:\n",
    "\n",
    "1. Comment structurer un projet de modélisation prédictive de bout en bout.\n",
    "2. Comment mapper les tâches que vous avez apprises dans le cours sur un projet.\n",
    "3. Comment utiliser au mieux le modèle de projet structuré pour garantir un résultat précis pour votre\n",
    "jeux de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est extrêmement important de résoudre les problèmes d'apprentissage automatique de bout en bout. Vous pouvez en lire plus sur l'apprentissage automatique. Vous pouvez également reproduire de petites recettes prêtes. Mais l'apprentissage automatique appliqué ne prendra vie que lorsque vous travaillerez sur un ensemble de données du début à la fin.\n",
    "\n",
    "Travailler sur un projet vous oblige à réfléchir à la façon dont le modèle sera utilisé, à remettre en question vos hypothèses et à vous améliorer dans toutes les parties d'un projet, pas seulement vos parties préférées. La meilleure façon de mettre en pratique des projets d'apprentissage automatique de modélisation prédictive consiste à utiliser des ensembles de données standardisés du référentiel UCI Machine Learning. Une fois que vous avez un ensemble de données de pratique et un tas de recettes Python, comment mettre tout cela ensemble et résoudre le problème de bout en bout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisez un processus structuré étape par étape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout projet d'apprentissage automatique de modélisation prédictive peut être divisé en six tâches courantes:\n",
    "1. Définissez le problème.\n",
    "2. Résumez les données.\n",
    "3. Préparez les données.\n",
    "4. Évaluer les algorithmes.\n",
    "5. Améliorez les résultats.\n",
    "6. Présentez les résultats.\n",
    "\n",
    "Ces tâches peuvent être combinées ou décomposées davantage, mais c'est la structure générale. Pour résoudre les problèmes d'apprentissage automatique de la modélisation prédictive en Python, vous devez mapper Python sur ce processus. Les tâches devront peut-être être adaptées ou légèrement renommées pour s'adapter à la façon de faire de Python (par exemple, Pandas pour le chargement de données et scikit-learn pour la modélisation). La section suivante fournit exactement ce mappage et élabore chaque tâche et les types de sous-tâches et de bibliothèques que vous pouvez utiliser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de projet d'apprentissage automatique en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section présente un modèle de projet que vous pouvez utiliser pour résoudre les problèmes d'apprentissage automatique en Python de bout en bout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape consiste à charger tout ce dont vous avez besoin pour commencer à travailler sur votre problème. Ceci comprend:\n",
    "- Modules, classes et fonctions Python que vous avez l'intention d'utiliser.\n",
    "- Chargement de votre jeu de données à partir de CSV.\n",
    "C'est également le siège de toute configuration globale que vous pourriez avoir à faire. C'est également l'endroit où vous devrez peut-être créer un échantillon réduit de votre jeu de données s'il est trop volumineux pour travailler.\n",
    "Idéalement, votre jeu de données doit être suffisamment petit pour créer un modèle ou créer une visualisation en une minute, idéalement 30 secondes. Vous pouvez toujours mettre à l'échelle des modèles performants plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGLISH VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT DESCRIPTION\n",
    "\n",
    "WHY ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you work end-to-end on a predictive modeling machine learning problem? In this project,\n",
    "you will work on a predictive classification modeling problem in Python, including every step of\n",
    "the applied machine learning process.\n",
    "After completing this project, you will know:\n",
    "- How to deal with an end-to-end classification predictive modeling problem.\n",
    "- How to use data transformations to improve model performance.\n",
    "- How to use algorithm tuning to improve model performance.\n",
    "- How to use the set methods and the tuning of the ensemble methods to improve the performance\n",
    "of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Problem I: Binary Classification Machine Learning Case Study Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on the Sonar Mines vs Rocks dataset. The problem is to predict metal or\n",
    "rocky objects from sonar return data. Each pattern is a set of 60 numbers between 0.0 and 1.0.\n",
    "Each number represents the energy in a particular frequency band, integrated over a period. The\n",
    "label associated with each instance contains the letter R if the object is a rock and M if it is a mine\n",
    "(metal cylinder).\n",
    "\n",
    "\n",
    "The dataset is available on Teams.\n",
    "Also available on:\n",
    "https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# saving the Model\n",
    "from pickle import dump\n",
    "\n",
    "# library for loading saved Model\n",
    "from pickle import load\n",
    "\n",
    "# sklearn object for precessing our model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# sklearn Algorithms for evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#sklearn Algorithms to improve Accuracy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Finalize model by checking how accurate is our model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To plot the model inside the jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# b) Load dataset\n",
    "filename = 'sonar.all-data'\n",
    "df = pd.read_csv(filename, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summarize Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape consiste à mieux comprendre les données dont vous disposez. Cela inclut la compréhension de vos données en utilisant:\n",
    "- Statistiques descriptives telles que des résumés.\n",
    "- Visualisations de données avec Matplotlib (ou autre), utilisant idéalement les fonctions pratiques de Pandas.\n",
    "Prenez votre temps et utilisez les résultats pour susciter de nombreuses questions, suppositions et hypothèses que vous pourrez étudier plus tard avec des modèles spécialisés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.1344</td>\n",
       "      <td>0.2041</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.1883</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.1605</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "158  0.0107  0.0453  0.0289  0.0713  0.1075  0.1019  0.1606  0.2119  0.3061   \n",
       "35   0.0206  0.0132  0.0533  0.0569  0.0647  0.1432  0.1344  0.2041  0.1571   \n",
       "191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n",
       "49   0.0119  0.0582  0.0623  0.0600  0.1397  0.1883  0.1422  0.1447  0.0487   \n",
       "92   0.0260  0.0192  0.0254  0.0061  0.0352  0.0701  0.1263  0.1080  0.1523   \n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "173  0.0329  0.0216  0.0386  0.0627  0.1158  0.1482  0.2054  0.1605  0.2532   \n",
       "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "158  0.2936  ...  0.0164  0.0120  0.0113  0.0021  0.0097  0.0072  0.0060   \n",
       "35   0.1573  ...  0.0386  0.0147  0.0018  0.0100  0.0096  0.0077  0.0180   \n",
       "191  0.1533  ...  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054  0.0035   \n",
       "49   0.0864  ...  0.0025  0.0103  0.0074  0.0123  0.0069  0.0076  0.0073   \n",
       "92   0.1630  ...  0.0118  0.0120  0.0051  0.0070  0.0015  0.0035  0.0008   \n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "173  0.2672  ...  0.0095  0.0151  0.0059  0.0015  0.0053  0.0016  0.0042   \n",
       "198  0.2048  ...  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028  0.0013   \n",
       "\n",
       "         58      59  60  \n",
       "158  0.0017  0.0036   M  \n",
       "35   0.0109  0.0070   R  \n",
       "191  0.0001  0.0055   M  \n",
       "49   0.0030  0.0138   R  \n",
       "92   0.0044  0.0077   R  \n",
       "0    0.0090  0.0032   R  \n",
       "205  0.0077  0.0031   M  \n",
       "1    0.0052  0.0044   R  \n",
       "173  0.0053  0.0074   M  \n",
       "198  0.0035  0.0060   M  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape consiste à préparer les données de manière à exposer au mieux la structure du problème et les relations entre vos attributs d'entrée et la variable de sortie. Cela comprend des tâches telles que:\n",
    "- Nettoyage des données en supprimant les doublons, en marquant les valeurs manquantes et même en imputant les valeurs manquantes.\n",
    "- Sélection de caractéristiques où les caractéristiques redondantes peuvent être supprimées et de nouvelles caractéristiques développées.\n",
    "- Transformation de données où les attributs sont mis à l'échelle ou redistribués afin de mieux exposer la structure du problème plus tard aux algorithmes d'apprentissage.\n",
    "\n",
    "Démarrage simple. Revenez souvent sur cette étape et passez à l'étape suivante jusqu'à ce que vous convergiez vers un sous-ensemble d'algorithmes et une présentation des données qui aboutit à des modèles précis ou suffisamment précis pour continuer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Data Cleaning (Missing values, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there any missing values?\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.03430</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.04405</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.4264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.3823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.10695</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.3729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.15225</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.18240</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.7106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.22480</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.7342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.24905</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.26395</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.28110</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.28170</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.30470</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.30840</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.36830</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.43495</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.54250</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.61770</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.66490</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.69970</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.69850</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.72110</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.75450</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.74560</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.73190</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.68080</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.60715</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.49035</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.42960</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.9306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.39120</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.35105</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.31275</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.32115</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.30630</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.31270</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.28350</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.9857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.27805</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.9297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.25950</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.24510</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.8246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.22255</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.7762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.14800</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.12135</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.7292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.10165</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.5522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.07810</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.3339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.04470</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.01790</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.0825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.01390</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.0709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.00955</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.0447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.00595</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.00580</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.0440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.00640</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.0439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count      mean       std     min       25%      50%       75%     max\n",
       "0   208.0  0.029164  0.022991  0.0015  0.013350  0.02280  0.035550  0.1371\n",
       "1   208.0  0.038437  0.032960  0.0006  0.016450  0.03080  0.047950  0.2339\n",
       "2   208.0  0.043832  0.038428  0.0015  0.018950  0.03430  0.057950  0.3059\n",
       "3   208.0  0.053892  0.046528  0.0058  0.024375  0.04405  0.064500  0.4264\n",
       "4   208.0  0.075202  0.055552  0.0067  0.038050  0.06250  0.100275  0.4010\n",
       "5   208.0  0.104570  0.059105  0.0102  0.067025  0.09215  0.134125  0.3823\n",
       "6   208.0  0.121747  0.061788  0.0033  0.080900  0.10695  0.154000  0.3729\n",
       "7   208.0  0.134799  0.085152  0.0055  0.080425  0.11210  0.169600  0.4590\n",
       "8   208.0  0.178003  0.118387  0.0075  0.097025  0.15225  0.233425  0.6828\n",
       "9   208.0  0.208259  0.134416  0.0113  0.111275  0.18240  0.268700  0.7106\n",
       "10  208.0  0.236013  0.132705  0.0289  0.129250  0.22480  0.301650  0.7342\n",
       "11  208.0  0.250221  0.140072  0.0236  0.133475  0.24905  0.331250  0.7060\n",
       "12  208.0  0.273305  0.140962  0.0184  0.166125  0.26395  0.351250  0.7131\n",
       "13  208.0  0.296568  0.164474  0.0273  0.175175  0.28110  0.386175  0.9970\n",
       "14  208.0  0.320201  0.205427  0.0031  0.164625  0.28170  0.452925  1.0000\n",
       "15  208.0  0.378487  0.232650  0.0162  0.196300  0.30470  0.535725  0.9988\n",
       "16  208.0  0.415983  0.263677  0.0349  0.205850  0.30840  0.659425  1.0000\n",
       "17  208.0  0.452318  0.261529  0.0375  0.242075  0.36830  0.679050  1.0000\n",
       "18  208.0  0.504812  0.257988  0.0494  0.299075  0.43495  0.731400  1.0000\n",
       "19  208.0  0.563047  0.262653  0.0656  0.350625  0.54250  0.809325  1.0000\n",
       "20  208.0  0.609060  0.257818  0.0512  0.399725  0.61770  0.816975  1.0000\n",
       "21  208.0  0.624275  0.255883  0.0219  0.406925  0.66490  0.831975  1.0000\n",
       "22  208.0  0.646975  0.250175  0.0563  0.450225  0.69970  0.848575  1.0000\n",
       "23  208.0  0.672654  0.239116  0.0239  0.540725  0.69850  0.872175  1.0000\n",
       "24  208.0  0.675424  0.244926  0.0240  0.525800  0.72110  0.873725  1.0000\n",
       "25  208.0  0.699866  0.237228  0.0921  0.544175  0.75450  0.893800  1.0000\n",
       "26  208.0  0.702155  0.245657  0.0481  0.531900  0.74560  0.917100  1.0000\n",
       "27  208.0  0.694024  0.237189  0.0284  0.534775  0.73190  0.900275  1.0000\n",
       "28  208.0  0.642074  0.240250  0.0144  0.463700  0.68080  0.852125  1.0000\n",
       "29  208.0  0.580928  0.220749  0.0613  0.411400  0.60715  0.735175  1.0000\n",
       "30  208.0  0.504475  0.213992  0.0482  0.345550  0.49035  0.641950  0.9657\n",
       "31  208.0  0.439040  0.213237  0.0404  0.281400  0.42960  0.580300  0.9306\n",
       "32  208.0  0.417220  0.206513  0.0477  0.257875  0.39120  0.556125  1.0000\n",
       "33  208.0  0.403233  0.231242  0.0212  0.217575  0.35105  0.596125  0.9647\n",
       "34  208.0  0.392571  0.259132  0.0223  0.179375  0.31275  0.593350  1.0000\n",
       "35  208.0  0.384848  0.264121  0.0080  0.154350  0.32115  0.556525  1.0000\n",
       "36  208.0  0.363807  0.239912  0.0351  0.160100  0.30630  0.518900  0.9497\n",
       "37  208.0  0.339657  0.212973  0.0383  0.174275  0.31270  0.440550  1.0000\n",
       "38  208.0  0.325800  0.199075  0.0371  0.173975  0.28350  0.434900  0.9857\n",
       "39  208.0  0.311207  0.178662  0.0117  0.186450  0.27805  0.424350  0.9297\n",
       "40  208.0  0.289252  0.171111  0.0360  0.163100  0.25950  0.387525  0.8995\n",
       "41  208.0  0.278293  0.168728  0.0056  0.158900  0.24510  0.384250  0.8246\n",
       "42  208.0  0.246542  0.138993  0.0000  0.155200  0.22255  0.324525  0.7733\n",
       "43  208.0  0.214075  0.133291  0.0000  0.126875  0.17770  0.271750  0.7762\n",
       "44  208.0  0.197232  0.151628  0.0000  0.094475  0.14800  0.231550  0.7034\n",
       "45  208.0  0.160631  0.133938  0.0000  0.068550  0.12135  0.200375  0.7292\n",
       "46  208.0  0.122453  0.086953  0.0000  0.064250  0.10165  0.154425  0.5522\n",
       "47  208.0  0.091424  0.062417  0.0000  0.045125  0.07810  0.120100  0.3339\n",
       "48  208.0  0.051929  0.035954  0.0000  0.026350  0.04470  0.068525  0.1981\n",
       "49  208.0  0.020424  0.013665  0.0000  0.011550  0.01790  0.025275  0.0825\n",
       "50  208.0  0.016069  0.012008  0.0000  0.008425  0.01390  0.020825  0.1004\n",
       "51  208.0  0.013420  0.009634  0.0008  0.007275  0.01140  0.016725  0.0709\n",
       "52  208.0  0.010709  0.007060  0.0005  0.005075  0.00955  0.014900  0.0390\n",
       "53  208.0  0.010941  0.007301  0.0010  0.005375  0.00930  0.014500  0.0352\n",
       "54  208.0  0.009290  0.007088  0.0006  0.004150  0.00750  0.012100  0.0447\n",
       "55  208.0  0.008222  0.005736  0.0004  0.004400  0.00685  0.010575  0.0394\n",
       "56  208.0  0.007820  0.005785  0.0003  0.003700  0.00595  0.010425  0.0355\n",
       "57  208.0  0.007949  0.006470  0.0003  0.003600  0.00580  0.010350  0.0440\n",
       "58  208.0  0.007941  0.006181  0.0001  0.003675  0.00640  0.010325  0.0364\n",
       "59  208.0  0.006507  0.005031  0.0006  0.003100  0.00530  0.008525  0.0439"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60\n",
       "M    111\n",
       "R     97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Breakdown of the data by the class variable (60) \n",
    "# For more you can read here: https://www.analyticsvidhya.com/blog/2020/03/understanding-transform-function-python/\n",
    "df.groupby(60).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Mines vs Rocks ~ M and R with 1 and 0 (required by the StandardScaler method)\n",
    "df[60].replace('M', 1, inplace=True)\n",
    "df[60].replace('R', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.2668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.1787</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.1972</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.1806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>0.2555</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8       9   ...      51  \\\n",
       "12   0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009  0.1240  ...  0.0176   \n",
       "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896  0.2668  ...  0.0088   \n",
       "8    0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684  0.1487  ...  0.0145   \n",
       "15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459  0.0852  ...  0.0031   \n",
       "146  0.1021  0.0830  0.0577  0.0627  0.0635  0.1328  0.0988  0.1787  0.1199  0.1369  ...  0.0709   \n",
       "187  0.0368  0.0279  0.0103  0.0566  0.0759  0.0679  0.0970  0.1473  0.2164  0.2544  ...  0.0105   \n",
       "52   0.0087  0.0046  0.0081  0.0230  0.0586  0.0682  0.0993  0.0717  0.0576  0.0818  ...  0.0052   \n",
       "86   0.0188  0.0370  0.0953  0.0824  0.0249  0.0488  0.1424  0.1972  0.1873  0.1806  ...  0.0093   \n",
       "138  0.0731  0.1249  0.1665  0.1496  0.1443  0.2770  0.2555  0.1712  0.0466  0.1114  ...  0.0444   \n",
       "193  0.0203  0.0121  0.0380  0.0128  0.0537  0.0874  0.1021  0.0852  0.1136  0.1747  ...  0.0134   \n",
       "\n",
       "         52      53      54      55      56      57      58      59  60  \n",
       "12   0.0127  0.0088  0.0098  0.0019  0.0059  0.0058  0.0059  0.0032   0  \n",
       "28   0.0104  0.0036  0.0088  0.0047  0.0117  0.0020  0.0091  0.0058   0  \n",
       "8    0.0128  0.0145  0.0058  0.0049  0.0065  0.0093  0.0059  0.0022   0  \n",
       "15   0.0153  0.0071  0.0212  0.0076  0.0152  0.0049  0.0200  0.0073   0  \n",
       "146  0.0317  0.0309  0.0252  0.0087  0.0177  0.0214  0.0227  0.0106   1  \n",
       "187  0.0024  0.0018  0.0057  0.0092  0.0009  0.0086  0.0110  0.0052   1  \n",
       "52   0.0038  0.0079  0.0114  0.0050  0.0030  0.0064  0.0058  0.0030   0  \n",
       "86   0.0033  0.0113  0.0030  0.0057  0.0090  0.0057  0.0068  0.0024   0  \n",
       "138  0.0230  0.0290  0.0141  0.0161  0.0177  0.0194  0.0207  0.0057   1  \n",
       "193  0.0094  0.0047  0.0045  0.0042  0.0028  0.0036  0.0013  0.0016   1  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let Check the 10 top data\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape consiste à trouver un sous-ensemble d'algorithmes d'apprentissage automatique capables d'exploiter la structure de vos données (par exemple, avoir des performances supérieures à la moyenne). Cela implique des étapes telles que:\n",
    "- Séparer un ensemble de données de validation à utiliser pour une confirmation ultérieure de la performance de votre modèle développé.\n",
    "- Définition des options de test à l'aide de scikit-learn telles que la validation croisée (Cross validation) et la métrique (accuracy, precision, recall, sensitivity, etc...) d'évaluation à utiliser.\n",
    "- Vérification ponctuelle d'une suite d'algorithmes d'apprentissage automatique linéaires et non linéaires.\n",
    "- Comparaison de la précision estimée des algorithmes.\n",
    "Sur un problème donné, vous passerez probablement la plupart de votre temps à cette étape et à l'étape précédente jusqu'à ce que vous convergiez vers un ensemble d'algorithmes d'apprentissage automatique, 3 à 5, performants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Split-out validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and validation datasets, We will use 80% of the dataset for modeling and 20% for validation.\n",
    "arr = df.values\n",
    "X = arr[:,0:60]\n",
    "y = arr[:,60]\n",
    "test_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Test options and evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Spot Check Algorithms\n",
    "# d) Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 76.94852941176471 (10.05102950966478)\n",
      "LDA: 74.63235294117648 (11.785367885381074)\n",
      "KNN: 80.80882352941175 (6.750704820308338)\n",
      "CART: 75.25735294117646 (10.405191199239615)\n",
      "NB: 64.88970588235294 (14.18684214516758)\n",
      "SVM: 77.64705882352942 (9.041128152498237)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAMCCAYAAACyT/OAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApAElEQVR4nO3df7jmdX3f+de7M/xYf4BnCokNoLgJsVCN2kzMJquJ5icxSYmJjRC7/rgI1Gyd9NJuF1fcgG0hSa8VkxLdWRKstamgSTQhXaxmG42SZluGFF0RNYhRJsQGnDEoCAJ+9o/7Hr05npk5wDnne96cx+O6uDj3/f3e9/2+73Pf9znP8/1+76kxRgAAADr5G1MPAAAA8GAJGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAG6yq3lJV/2KdrvtFVfXeQyx/TlXtXY/b7q6qXlNVvzH1HACsjpABWCdV9f6q2l9VR23UbY4x/t0Y44cWZhhV9S0bdfs18/NV9ZGqurOq9lbVb1XVUzdqhodqjHHxGONnp54DgNURMgDroKpOTvLsJCPJ39ug29y+EbdzGL+a5B8n+fkkO5J8a5LfTfKjE850WJvksQPgQRAyAOvjxUn+3yRvSfKSQ61YVf9rVf1lVd1aVT+7uBWlqo6tqrdW1W1V9emqem1V/Y35spdW1R9X1Ruqal+SC+fnXTNf/oH5TXyoqr5YVS9cuM1/UlV/Nb/dly2c/5aqelNVvXt+mT+uqsdX1a/Mty59rKqecZD7cUqSf5TkrDHGH44x7hlj3DXfSvRLD/L+fL6qbq6q756ff8t83pcsm3V3Vf1BVX2hqv6oqp64sPxX55e7o6quq6pnLyy7sKp+u6p+s6ruSPLS+Xm/OV9+9HzZ5+azXFtV3zhf9k1VdVVV7auqm6rqnGXX+475ffxCVd1QVTsP9f0H4KERMgDr48VJ/t38vx8+8EvwclV1epJXJfmBJN+S5HuXrXJpkmOT/PfzZS9O8rKF5d+Z5OYk35DkosULjjG+Z/7l08YYjxljvH1++vHz6zwhydlJ3lhVSwsX/ekkr01yXJJ7kvxJkj+dn/7tJJcc5D5/f5K9Y4z/cpDlq70/H07yN5O8LcmVSb4js8fmHyT5tap6zML6L0ryz+ezXZ/Z433AtUmentmWobcl+a2qOnph+Rnz+/O4ZZdLZvF5bJKT5rO8PMmX5suuSLI3yTcleUGSi6vq+xcu+/fmcz8uyVVJfu3gDwcAD5WQAVhjVfWsJE9M8o4xxnVJPpnkZw6y+k8n+ddjjBvGGHcled3C9WxL8sIk/9sY4wtjjD9P8vok/9PC5W8dY1w6xrhvjPGlrM69Sf7ZGOPeMcbVSb6Y5MkLy981xrhujHF3kncluXuM8dYxxv1J3p5kxS0ymf3C/5cHu9FV3p9PjTH+9cJtnTSf9Z4xxnuTfDmzqDng/x5jfGCMcU+S85N8V1WdlCRjjN8cY3xu/ti8PslRy+7nn4wxfneM8ZUVHrt75/fnW8YY988fjzvm1/2sJOeNMe4eY1yf5DeW3YdrxhhXz+/Dv03ytIM9JgA8dEIGYO29JMl7xxi3z0+/LQffveybktyycHrx6+OSHJnk0wvnfTqzLSkrrb9anxtj3Ldw+q4ki1s5/tvC119a4fTiug+43iR/6xC3u5r7s/y2MsY41O1/9f6PMb6YZF9mj+mB3edurKq/rqrPZ7aF5biVLruCf5vkPUmunO/y9y+r6oj5de8bY3zhEPfhswtf35XkaMfgAKw9IQOwhqrqv8tsK8v3VtVnq+qzSV6Z5GlVtdJf5v8yyYkLp09a+Pr2zLYMPHHhvCck+YuF02NNBl8b/zHJiYc4JmQ19+fB+urjNd/lbEeSW+fHw5yX2fdiaYzxuCR/naQWLnvQx26+tep1Y4zTknx3kh/LbDe4W5PsqKrHruF9AOAhEDIAa+snktyf5LTMjs94epJTk3wws1+El3tHkpdV1alV9agkv3BgwXzXpHckuaiqHjs/kP1VSX7zQczz3zI7HmXdjTH+LMmbklxRs3+v5sj5QfNnVtWr1+j+LPe8qnpWVR2Z2bEy/3mMcUuSxya5L8ltSbZX1S8kOWa1V1pVz62qp853h7sjswC7f37d/ynJL87v27dldpzR8mNsAFhnQgZgbb0ks2NePjPG+OyB/zI74PtFy3cxGmO8O8m/SvK+JDdldmB9MjvIPkl2JbkzswP6r8lsN7U3P4h5Lkzyb+afvPXTD/E+PRg/n9l9fWOSz2d2fNDzk/z+fPnDvT/LvS3JBZntUvbtmR38n8x2C3t3kk9ktuvX3Xlwu+E9PrMPArgjyY1J/ihfC66zkpyc2daZdyW5YIzxBw/jPgDwENQYm2mvBICtrapOTfKRJEctO46FZarqLZl9Stprp54FgI1niwzAxKrq+fPdsJaS/HKS3xcxAHBoQgZgev8ws2M5PpnZ8TU/N+04ALD52bUMAABoxxYZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7Wyf6oaPO+64cfLJJ0918wAAwCZ33XXX3T7GOH6lZZOFzMknn5w9e/ZMdfMAAMAmV1WfPtgyu5YBAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHa2Tz0AADySVNXUI2SMMfUIQDbH+0HyyH1PsEUGYGJXXHFFnvKUp2Tbtm15ylOekiuuuGLqkXgYxhgP67+1ug5gepvh/eCR/J5giwzAhK644oqcf/75ufzyy/OsZz0r11xzTc4+++wkyVlnnTXxdACwedkiAzChiy66KJdffnme+9zn5ogjjshzn/vcXH755bnoooumHg0ANrWaanPTzp07x549eya5bYDNYtu2bbn77rtzxBFHfPW8e++9N0cffXTuv//+CSdjKlX1iN4VBFg97wdJVV03xti50jJbZAAmdOqpp+aaa655wHnXXHNNTj311IkmAoAehAzAhM4///ycffbZed/73pd7770373vf+3L22Wfn/PPPn3o0ANjUHOwPMKEDB/Tv2rUrN954Y0499dRcdNFFDvQHgMNwjAwAbCL2iQcO8H7gGBkAAOARRsgAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0M6qQqaqTq+qj1fVTVX16hWWL1XVu6rqw1X1X6rqKWs/KgAAwMxhQ6aqtiV5Y5IfSXJakrOq6rRlq70myfVjjG9L8uIkv7rWgwIAABywmi0yz0xy0xjj5jHGl5NcmeSMZeucluQ/JskY42NJTq6qb1zTSQEAAOZWEzInJLll4fTe+XmLPpTkJ5Okqp6Z5IlJTlyLAQEAAJZbTcjUCueNZad/KclSVV2fZFeS/5rkvq+7oqpzq2pPVe257bbbHuysAAAASZLtq1hnb5KTFk6fmOTWxRXGGHckeVmSVFUl+dT8vyxb77IklyXJzp07l8cQAADAqqxmi8y1SU6pqidV1ZFJzkxy1eIKVfW4+bIk+dkkH5jHDQAAwJo77BaZMcZ9VfWKJO9Jsi3Jm8cYN1TVy+fLdyc5Nclbq+r+JB9NcvY6zgwAAGxxq9m1LGOMq5Ncvey83Qtf/0mSU9Z2NAAAgJWt6h/EBAAA2EyEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtbJ96gM6qauoRkiRjjKlH2NI8D0g8DwBgowmZh+Hh/sJQVX7peARYi++h50J/ngcAsLGEDFvejh07sn///qnHmPQv+ktLS9m3b99ktw8A8GAJGba8/fv3b/m/gm+W3aIAAFbLwf4AAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO1sn3qAKe3YsSP79++fdIaqmvT2l5aWsm/fvklnAACAB2tLh8z+/fszxph6jElNHVIAAPBQ2LUMAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALSzfeoBADaDHTt2ZP/+/VOPkaqa7LaXlpayb9++yW4fHkmmfC0fMMaYeoT2NsPPhqmfS5v5Z4OQAUiyf//+Lf9Df+oflvBI8nDfT6pqy78nbQZ+Nmzunw12LQMAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDvbpx4AYDMYFxyTXHjs1GNMalxwzNQjAMCqCRmAJPW6OzLGmHqMSVVVxoVTTwEAq2PXMgAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhnVSFTVadX1cer6qaqevUKy4+tqt+vqg9V1Q1V9bK1HxUAAGDmsCFTVduSvDHJjyQ5LclZVXXastX+UZKPjjGeluQ5SV5fVUeu8awAAABJVrdF5plJbhpj3DzG+HKSK5OcsWydkeSxVVVJHpNkX5L71nRSAACAue2rWOeEJLcsnN6b5DuXrfNrSa5KcmuSxyZ54RjjK2syIQBskB07dmT//v1Tj5HZ3wWnsbS0lH379k12+wCrtZqQWenddCw7/cNJrk/yfUm+OckfVNUHxxh3POCKqs5Ncm6SPOEJT3jQwwLAetq/f3/GWP4jbmuZMqIAHozV7Fq2N8lJC6dPzGzLy6KXJXnnmLkpyaeS/O3lVzTGuGyMsXOMsfP4449/qDMDAABb3GpC5tokp1TVk+YH8J+Z2W5kiz6T5PuTpKq+McmTk9y8loMCAAAccNhdy8YY91XVK5K8J8m2JG8eY9xQVS+fL9+d5J8neUtV/X+Z7Yp23hjj9nWcGwAA2MJWc4xMxhhXJ7l62Xm7F76+NckPre1oAAAAK1vVP4gJAACwmQgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7WyfegAAANiMxgXHJBceO/UYkxoXHDP1CAclZAAAYAX1ujsyxph6jElVVcaFU0+xMruWAQAA7QgZAACgHSEDAAC0I2QAAIB2tvTB/j6JYnN/EgUAABzMlg4Zn0SxuT+JAgAADsauZQAAQDtCBgAAaGdL71oGsKiqph5hUktLS1OPAACrJmQAkk1xvFxVbYo5AKADu5YBAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALSzfeoBAB4JqmpTXM8YY03m2KrGBcckFx479RiTGhccM/UIAKsiZADWgIB4ZKjX3bHlv5dVlXHh1FMAHJ5dywAAgHaEDAAA0I5dywAAltmxY0f2798/6QxrdezdQ7W0tJR9+/ZNOgMcipABAFhm//79jpeaOKTgcOxaBgAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdrZPPQBMbVxwTHLhsVOPMalxwTFTjwAA8KAIGba8et0dGWNMPcakqirjwqmnAABYPbuWAQAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZgInt2rUrRx99dKoqRx99dHbt2jX1SACw6QkZgAnt2rUru3fvzsUXX5w777wzF198cXbv3i1mAOAwaowxyQ3v3Llz7NmzZ5LbPqCqMtX93yw8Bh6DxGMwpaOPPjoXX3xxXvWqV331vEsuuSSvec1rcvfdd0842dbkteAxOMDj4DFIPAbJ9I9BVV03xti54rKtHjJb3dLSUvbt2zf1GJOa+gW6GXgMplNVufPOO/OoRz3qq+fdddddefSjH+17MgGvBY/BAR4Hj0HiMUimfwwOFTJbeteyMcak/22GGbZ6xMDUjjrqqOzevfsB5+3evTtHHXXURBMBQA+rCpmqOr2qPl5VN1XVq1dY/k+r6vr5fx+pqvurasfajwvwyHLOOefkvPPOyyWXXJK77rorl1xySc4777ycc845U48GAJvaYXctq6ptST6R5AeT7E1ybZKzxhgfPcj6P57klWOM7zvU9W6GXcumNvWmOmZ8HzwGU9u1a1d+/dd/Pffcc0+OOuqonHPOObn00kunHmtL8lrwGBzgcfAYJB6DZPrH4FC7lm1fxeWfmeSmMcbN8yu7MskZSVYMmSRnJbnioQwKsBVdeumlwgUAHqTV7Fp2QpJbFk7vnZ/3darqUUlOT/I7D380AACAla0mZFb6aK+DbV/68SR/PMZY8Qjyqjq3qvZU1Z7bbrtttTMCAAA8wGpCZm+SkxZOn5jk1oOse2YOsVvZGOOyMcbOMcbO448/fvVTAgAALFhNyFyb5JSqelJVHZlZrFy1fKWqOjbJ9yb5vbUdEQAA4IEOe7D/GOO+qnpFkvck2ZbkzWOMG6rq5fPlB/4BhOcnee8Y4851mxYAYAOMC45JLjx26jEmNS44ZuoR4JAO+/HL68XHL0//cXbM+D54DOAArwWPwQEeB49B4jFIpn8MDvXxy6v6BzEBAAA2EyEDAAC0I2QAAIB2DnuwPwBsJVUr/fNpW8fS0tLUI8Cm4j1h874nCBkAmNsMB/VOfWAt8DVTvxa9HxyaXcsAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaGf71APAZlBVU48wqaWlpalHAIBHnLX4/WItrmOM8bCvYzMSMmx5m+HFXVWbYg4AYO342b6+7FoGAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANpZVchU1elV9fGquqmqXn2QdZ5TVddX1Q1V9UdrOyYAAMDXbD/cClW1Lckbk/xgkr1Jrq2qq8YYH11Y53FJ3pTk9DHGZ6rqG9ZpXgAAgFVtkXlmkpvGGDePMb6c5MokZyxb52eSvHOM8ZkkGWP81dqOCQAA8DWrCZkTktyycHrv/LxF35pkqareX1XXVdWLV7qiqjq3qvZU1Z7bbrvtoU0MAABseasJmVrhvLHs9PYk357kR5P8cJL/vaq+9esuNMZlY4ydY4ydxx9//IMeFgAAIFnFMTKZbYE5aeH0iUluXWGd28cYdya5s6o+kORpST6xJlMCAAAsWM0WmWuTnFJVT6qqI5OcmeSqZev8XpJnV9X2qnpUku9McuPajgoAADBz2C0yY4z7quoVSd6TZFuSN48xbqiql8+X7x5j3FhV/yHJh5N8JclvjDE+sp6DAwAAW1eNsfxwl42xc+fOsWfPnklue7Ooqkz1+LO5eC4AB3g/2Bx8HzwGbA5Vdd0YY+dKy1b1D2ICAABsJkIGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoZ/vUA3RWVZviOsYYD/s6eOjW4nu4FtfjeQCbw2b42eD9ANgKhMzD4AcFiecB8EDeEwA2hl3LAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdrZPPQAAwGZUVVOPMKmlpaWpR4BDEjIAAMuMMSa9/aqafAbY7OxaBgAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoJ1VhUxVnV5VH6+qm6rq1Sssf05V/XVVXT//7xfWflQAAICZ7Ydboaq2JXljkh9MsjfJtVV11Rjjo8tW/eAY48fWYUYAAIAHWM0WmWcmuWmMcfMY48tJrkxyxvqOBQAAcHCrCZkTktyycHrv/LzlvquqPlRV766qv7Mm0wEAAKzgsLuWJakVzhvLTv9pkieOMb5YVc9L8rtJTvm6K6o6N8m5SfKEJzzhwU0KAAAwt5otMnuTnLRw+sQkty6uMMa4Y4zxxfnXVyc5oqqOW35FY4zLxhg7xxg7jz/++IcxNgAAsJWtJmSuTXJKVT2pqo5McmaSqxZXqKrHV1XNv37m/Ho/t9bDAgAAJKvYtWyMcV9VvSLJe5JsS/LmMcYNVfXy+fLdSV6Q5Oeq6r4kX0py5hhj+e5nAAAAa6Km6o2dO3eOPXv2THLbAACbWVXF34Qhqarrxhg7V1q2qn8QEwAAYDMRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANCOkAEAANoRMgAAQDtCBgAAaEfIAAAA7QgZAACgHSEDAAC0I2QAAIB2hAwAANDOqkKmqk6vqo9X1U1V9epDrPcdVXV/Vb1g7UYEAAB4oMOGTFVtS/LGJD+S5LQkZ1XVaQdZ75eTvGethwQAAFi0mi0yz0xy0xjj5jHGl5NcmeSMFdbbleR3kvzVGs4HAADwdVYTMickuWXh9N75eV9VVSckeX6S3Ws3GgAAwMpWEzK1wnlj2elfSXLeGOP+Q15R1blVtaeq9tx2222rHBEAAOCBtq9inb1JTlo4fWKSW5etszPJlVWVJMcleV5V3TfG+N3FlcYYlyW5LEl27ty5PIYAAABWZTUhc22SU6rqSUn+IsmZSX5mcYUxxpMOfF1Vb0ny75dHDAAAwFo5bMiMMe6rqldk9mlk25K8eYxxQ1W9fL7ccTEAAMCGWs0WmYwxrk5y9bLzVgyYMcZLH/5YAAAAB7eqfxATAABgMxEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKAdIQMAALQjZAAAgHaEDAAA0I6QAQAA2hEyAABAO0IGAABoR8gAAADtCBkAAKCd7VMPAADwSFNVk1/HGONhzwCbmZABAFhjIgLWn13LAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQjpABAADaETIAAEA7NcaY5oarbkvy6UlufPM4LsntUw/BpuC5QOJ5wIznAYnnATOeB8kTxxjHr7RgspAhqao9Y4ydU8/B9DwXSDwPmPE8IPE8YMbz4NDsWgYAALQjZAAAgHaEzLQum3oANg3PBRLPA2Y8D0g8D5jxPDgEx8gAAADt2CIDAAC0I2Q2SFV9cYXzLqyqv6iq66vqo1V11hSzsX5W8X3/s6p6Z1WdtmydZ1TVqKof3rhpWS+Lz4Oqet78+/6E+XPhrqr6hoOsO6rq9Qun/5equnDDBmdNVNXjq+rKqvrk/L3+6qr61vmyV1bV3VV17ML6z6mqv66q/1pVH6uq/6Oqnjp/z7i+qvZV1afmX/8/090z1sKhXufLfl58rKr+z6ryu9sjRFWdX1U3VNWH59/jd1fVLy5b5+lVdeP86z+vqg8uW359VX1kI+feTLwYpveGMcbTk5yR5P+qqiMmnoeN8YYxxtPHGKckeXuSP6yqxc9IPyvJNfP/8whRVd+f5NIkp48xPjM/+/Yk/+QgF7knyU9W1XEbMR9rr6oqybuSvH+M8c1jjNOSvCbJN85XOSvJtUmev+yiHxxjPCPJM5L8WJJj5u8ZT09yVZJ/Oj/9AxtxP1hXh3udH/g94bQkT03yvRs1GOunqr4rs9f23x1jfFuSH0jyS0leuGzVM5O8beH0Y6vqpPl1nLoRs25mQmaTGGP8WZK7kixNPQsba4zx9iTvTfIzyVd/8XlBkpcm+aGqOnq66VgrVfXsJL+e5EfHGJ9cWPTmJC+sqh0rXOy+zA70fOUGjMj6eG6Se8cYuw+cMca4fozxwar65iSPSfLaHOSPFmOMLyW5PskJGzAr01jt6/zIJEcn2b/uE7ER/laS28cY9yTJGOP2McYfJfl8VX3nwno/neTKhdPvyNdi56wkV2zEsJuVkNkkqurvJvmzMcZfTT0Lk/jTJH97/vX/mORT819235/keVMNxZo5KsnvJfmJMcbHli37YmYx848Pctk3JnnR4q5HtPKUJNcdZNmBX0I+mOTJi7sYHlBVS0lOSfKBdZuQzeBQr/NXVtX1Sf4yySfGGNdv5GCsm/cmOamqPlFVb6qqA1varshsK0yq6n9I8rn5H7sP+O0kPzn/+seT/P5GDbwZCZnpvbKqPp7kPye5cOJZmE4tfH1WvvbXlytj97JHgnuT/KckZx9k+b9K8pKqOmb5gjHGHUnemuTn1288JnJmkivHGF9J8s4kf39h2bOr6sNJPpvk348xPjvFgGyMw7zOD+xa9g1JHl1VZ27kbKyPMcYXk3x7knOT3Jbk7VX10sx+7r9gfizUmfn6LS77kuyfPw9uzGxvni1LyEzvDWOMJ2e2mfCtdiPasp6R5Maq2pbkp5L8QlX9eWbHU/xIVT12yuF42L6S2e4B31FVr1m+cIzx+cz2gf6fD3L5X8ksgh69TvOxfm7I7JeVB6iqb8tsS8sfzF/rZ+aBf7T44Hy/+acm+bmqevr6j8rEfiWHeJ2PMe5N8h+SfM8GzsQ6GmPcP8Z4/xjjgiSvSPJTY4xbkvx5ZsdC/VRmu5It9/bMtuJt6d3KEiGzaYwx3plkT5KXTD0LG6uqfirJD2X2hvQDST40xjhpjHHyGOOJSX4nyU9MOCJrYIxxV2YHdr6oqlbaMnNJkn+YZPsKl92X2Q+zg23RYfP6wyRHVdU5B86oqu9I8qtJLpy/zk8eY3xTkhOq6omLFx5jfCLJLyY5byOHZuMd7nU+P37yu5N8cqXl9FJVT66qUxbOenqST8+/viLJG5J8coyxd4WLvyvJv0zynnUdsgEhs3EeVVV7F/571Qrr/LMkr/LRio8oB/u+v/LAxy8n+QdJvm+McVtmf5F917Lr+J3MPwiA3ua/qJye5LVVdcayZbdn9r0/6iAXf30Sn17WzJj9q9PPT/KD849fviGz3Yifk69/rb8r833jl9md5Huq6knrOCqbw0qv8wPHyHwksz90vGmjh2JdPCbJv5l/JPuHM/tUugvny34ryd/JAw/y/6oxxhfGGL88xvjyhky6idXsPRYAAKAPf/kHAADaETIAAEA7QgYAAGhHyAAAAO0IGQAAoB0hAwAAtCNkAACAdoQMAADQzv8PmQ/ZDNpdrL4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The baseline\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# using diff Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=200, multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='scale')))\n",
    "\n",
    "def eval_algorithms(models, show_boxplots=True):\n",
    "    # Evaluate each model in turn\n",
    "    # Setup the test harness to use 10-fold cross validation\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        kfold = KFold(n_splits=10, random_state=None)\n",
    "        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        #print(\"Estimated accuracy of {} with the mean of {} and std. dev. {}\".format(name, cv_results.mean()*100.0, cv_results.std()*100.0))\n",
    "        print(\"{}: {} ({})\".format(name, cv_results.mean()*100.0, cv_results.std()*100.0))\n",
    "    \n",
    "    # this if is for spoting the model\n",
    "    if show_boxplots:\n",
    "        # Create a plot of the model evaluation results to compae the spread \n",
    "        # and the estimated mean accuracy of each model\n",
    "        fig = plt.figure(figsize=(14,12)) \n",
    "        fig.suptitle('Algorithm Comparison') \n",
    "        ax = fig.add_subplot(111) \n",
    "        plt.boxplot(results) \n",
    "        ax.set_xticklabels(names) \n",
    "        plt.show()\n",
    "        \n",
    "eval_algorithms(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improve Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que vous avez une liste restreinte d'algorithmes d'apprentissage automatique, vous devez en tirer le meilleur parti.\n",
    "\n",
    "Il existe deux façons différentes d'améliorer la précision de vos modèles:\n",
    "\n",
    "- Recherchez une combinaison de paramètres pour chaque algorithme à l'aide de scikit-learn qui donne les meilleurs résultats.\n",
    "- Combinez la prédiction de plusieurs modèles en une prédiction d'ensemble à l'aide de techniques d'ensemble.\n",
    "\n",
    "La ligne entre cette étape et l'étape précédente peut être floue lorsqu'un projet devient concret.\n",
    "Il peut y avoir un petit réglage d'algorithme à l'étape précédente. Et dans le cas des techniques \"ensembles\", vous pouvez proposer plus qu'une liste restreinte d'algorithmes pour combiner leurs prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Imporove Accuracy we will use Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 74.04411764705883 (9.466751140841813)\n",
      "ScaledLDA: 74.63235294117648 (11.785367885381074)\n",
      "ScaledKNN: 82.57352941176471 (5.451103821426657)\n",
      "ScaledCART: 70.51470588235293 (9.269956495345893)\n",
      "ScaledNB: 64.88970588235294 (14.18684214516758)\n",
      "ScaledSVM: 83.63970588235293 (8.869747214968386)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline for Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(solver='lbfgs', max_iter=200, multi_class='ovr'))])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(gamma='scale'))])))\n",
    "\n",
    "eval_algorithms(pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve results with Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.85 using {'n_neighbors': 1}\n",
      "0.85 (0.05968577609876672) with: {'n_neighbors': 1}\n",
      "0.8371323529411765 (0.06601389343386418) with: {'n_neighbors': 3}\n",
      "0.8375 (0.03737725007982011) with: {'n_neighbors': 5}\n",
      "0.7639705882352941 (0.0893737825672642) with: {'n_neighbors': 7}\n",
      "0.7514705882352941 (0.08705087404087493) with: {'n_neighbors': 9}\n",
      "0.7334558823529411 (0.104830998652816) with: {'n_neighbors': 11}\n",
      "0.7330882352941176 (0.10580573208090603) with: {'n_neighbors': 13}\n",
      "0.7279411764705882 (0.07614821756326462) with: {'n_neighbors': 15}\n",
      "0.7099264705882353 (0.07928656388570995) with: {'n_neighbors': 17}\n",
      "0.7220588235294118 (0.08508785913176035) with: {'n_neighbors': 19}\n",
      "0.7102941176470587 (0.10950452661639593) with: {'n_neighbors': 21}\n"
     ]
    }
   ],
   "source": [
    "# KNN Algorithm tuning\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "k_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\n",
    "param_grid = dict(n_neighbors=k_values)\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{} ({}) with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8661764705882353 using {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.7584558823529413 (0.09948301158200631) with: {'C': 0.1, 'kernel': 'linear'}\n",
      "0.5294117647058824 (0.11882453042676783) with: {'C': 0.1, 'kernel': 'poly'}\n",
      "0.5731617647058824 (0.1309303654500583) with: {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.7040441176470589 (0.06693297175243644) with: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.7463235294117647 (0.10950699524194979) with: {'C': 0.3, 'kernel': 'linear'}\n",
      "0.6426470588235295 (0.13218739646981145) with: {'C': 0.3, 'kernel': 'poly'}\n",
      "0.7658088235294118 (0.09169164818973898) with: {'C': 0.3, 'kernel': 'rbf'}\n",
      "0.7349264705882353 (0.054566800193808336) with: {'C': 0.3, 'kernel': 'sigmoid'}\n",
      "0.7400735294117646 (0.08263557723261496) with: {'C': 0.5, 'kernel': 'linear'}\n",
      "0.6801470588235293 (0.09859523701095135) with: {'C': 0.5, 'kernel': 'poly'}\n",
      "0.788235294117647 (0.06418995518008062) with: {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.7466911764705882 (0.05874190108352445) with: {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.7466911764705882 (0.0841975980083792) with: {'C': 0.7, 'kernel': 'linear'}\n",
      "0.7400735294117646 (0.1279078937439239) with: {'C': 0.7, 'kernel': 'poly'}\n",
      "0.8125 (0.0855125246295074) with: {'C': 0.7, 'kernel': 'rbf'}\n",
      "0.7588235294117647 (0.06147955887260048) with: {'C': 0.7, 'kernel': 'sigmoid'}\n",
      "0.7588235294117647 (0.0965197821127894) with: {'C': 0.9, 'kernel': 'linear'}\n",
      "0.7702205882352942 (0.10251025879120317) with: {'C': 0.9, 'kernel': 'poly'}\n",
      "0.8363970588235293 (0.08869747214968386) with: {'C': 0.9, 'kernel': 'rbf'}\n",
      "0.7588235294117647 (0.06753507354826489) with: {'C': 0.9, 'kernel': 'sigmoid'}\n",
      "0.7525735294117647 (0.09888339070211641) with: {'C': 1.0, 'kernel': 'linear'}\n",
      "0.7882352941176471 (0.10841786472257907) with: {'C': 1.0, 'kernel': 'poly'}\n",
      "0.8363970588235293 (0.08869747214968386) with: {'C': 1.0, 'kernel': 'rbf'}\n",
      "0.7466911764705882 (0.07670442894915014) with: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.7698529411764706 (0.10608640468747901) with: {'C': 1.3, 'kernel': 'linear'}\n",
      "0.8183823529411764 (0.10715130136439335) with: {'C': 1.3, 'kernel': 'poly'}\n",
      "0.8481617647058822 (0.08041391757735976) with: {'C': 1.3, 'kernel': 'rbf'}\n",
      "0.7099264705882353 (0.0875810610679356) with: {'C': 1.3, 'kernel': 'sigmoid'}\n",
      "0.7580882352941176 (0.09202639910095302) with: {'C': 1.5, 'kernel': 'linear'}\n",
      "0.8301470588235293 (0.11025489498662683) with: {'C': 1.5, 'kernel': 'poly'}\n",
      "0.8661764705882353 (0.0914577019483583) with: {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.7404411764705883 (0.06276328627315675) with: {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "0.7463235294117647 (0.09041427146434937) with: {'C': 1.7, 'kernel': 'linear'}\n",
      "0.8305147058823529 (0.11670637248622173) with: {'C': 1.7, 'kernel': 'poly'}\n",
      "0.8602941176470589 (0.08828123803855142) with: {'C': 1.7, 'kernel': 'rbf'}\n",
      "0.7161764705882353 (0.09998107519887887) with: {'C': 1.7, 'kernel': 'sigmoid'}\n",
      "0.7584558823529411 (0.09406377709245813) with: {'C': 2.0, 'kernel': 'linear'}\n",
      "0.8308823529411764 (0.10895014494274426) with: {'C': 2.0, 'kernel': 'poly'}\n",
      "0.8661764705882353 (0.09516592230502519) with: {'C': 2.0, 'kernel': 'rbf'}\n",
      "0.7216911764705882 (0.10057751168328524) with: {'C': 2.0, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# Tune scaled SVM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "model = SVC(gamma='auto')\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold) \n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{} ({}) with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve results with ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB: 81.39705882352942 (6.601696463316956)\n",
      "GBM: 84.77941176470587 (10.687342544217408)\n",
      "RF: 82.53676470588236 (5.537331038921374)\n",
      "ET: 86.61764705882351 (7.701657463277687)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensembles\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier())) \n",
    "ensembles.append(('GBM', GradientBoostingClassifier())) \n",
    "ensembles.append(('RF', RandomForestClassifier(n_estimators=100))) \n",
    "ensembles.append(('ET', ExtraTreesClassifier(n_estimators=100))) \n",
    "\n",
    "eval_algorithms(ensembles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Finalize Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que vous avez trouvé un modèle qui, selon vous, peut faire des prédictions précises sur des données inconnues, vous êtes prêt à le finaliser. La finalisation d'un modèle peut impliquer des sous-tâches telles que:\n",
    "\n",
    "- Utilisation d'un modèle optimal optimisé par scikit-learn pour faire des prédictions sur des données inconnues.\n",
    "- Création d'un modèle autonome en utilisant les paramètres réglés par scikit-learn.\n",
    "- Sauvegarde d'un modèle optimal dans un fichier pour une utilisation ultérieure.\n",
    "\n",
    "Une fois que vous êtes arrivé à ce stade, vous êtes prêt à présenter les résultats aux parties prenantes et / ou à déployer votre modèle pour commencer à faire des prédictions sur des données inconnues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Predictions on validation dataset\n",
    "# b) Create standalone model on entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8571428571428571\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  2]\n",
      " [ 4 23]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.87      0.81        15\n",
      "         1.0       0.92      0.85      0.88        27\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.84      0.86      0.85        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "clf = SVC(C=1.5, kernel='rbf', gamma='auto')\n",
    "clf.fit(rescaledX, Y_train)\n",
    "\n",
    "# Estimate accuracy on test dataset\n",
    "rescaledTestX = scaler.transform(X_test)\n",
    "pred = clf.predict(rescaledTestX)\n",
    "\n",
    "print('Accuracy score: {}'.format(accuracy_score(Y_test, pred)))\n",
    "print('\\nConfusion matrix:\\n {}'.format(confusion_matrix(Y_test, pred)))\n",
    "print('\\nClassification report:\\n {}'.format(classification_report(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mines</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rocks</th>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1   0\n",
       "Mines  13   2\n",
       "Rocks   4  23"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mines vs Rocks\n",
    "cm = confusion_matrix(Y_test,pred)\n",
    "confusion = pd.DataFrame(cm, index=['Mines', 'Rocks'], columns=['1', '0'])\n",
    "cm1 = classification_report(Y_test,pred)\n",
    "cm\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Save the model to Hard disk, we mean on local machine\n",
    "filename = 'finalized_model.sav' \n",
    "dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# Some time later...\n",
    "# Load the model from hard disk\n",
    "loaded_clf = load(open(filename, 'rb'))\n",
    "\n",
    "# The data must be standardized as in original model training\n",
    "result = loaded_clf.score(rescaledTestX, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conseils pour bien utiliser le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section répertorie les astuces que vous pouvez utiliser pour tirer le meilleur parti du modèle de projet d'apprentissage automatique en Python.\n",
    "- Premier passage rapide. Faites un premier pas à travers les étapes du projet aussi vite que possible. Cela vous donnera l'assurance que vous disposez de toutes les pièces dont vous avez besoin et d'une base à partir de laquelle vous pouvez vous améliorer.\n",
    "- Cycles. Le processus n'est pas linéaire mais cyclique. Vous ferez une boucle entre les étapes et passerez probablement la plupart de votre temps dans des boucles serrées entre les étapes 3-4 ou 3-4-5 jusqu'à ce que vous atteigniez un niveau de précision suffisant ou que vous manquiez de temps.\n",
    "- Essayez chaque étape. Il est facile de sauter des étapes, surtout si vous n'êtes pas confiant ou familier avec les tâches de cette étape. Essayez de faire quelque chose à chaque étape du processus, même si cela n'améliore pas la précision. Vous pouvez toujours vous en inspirer plus tard. Ne sautez pas d'étapes, réduisez simplement leur contribution.\n",
    "- Précision du cliquet. Le but du projet est la précision du modèle. Chaque pas contribue à cet objectif. Traitez les modifications que vous apportez comme des expériences qui augmentent la précision comme le chemin en or du processus et réorganisez les autres étapes autour d'elles. La précision est un cliquet qui ne peut se déplacer que dans une seule direction (meilleure, pas pire).\n",
    "- Adaptez-vous au besoin. Modifiez les étapes selon vos besoins sur un projet, d'autant plus que vous devenez plus expérimenté avec le modèle. Adoucissez les bords des tâches, telles que les étapes 4 à 5, pour optimiser la précision du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END, Merci!!!."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
